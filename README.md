# Fine-Tuning LLM with Kubernetes

An end-to-end production-grade pipeline for fine-tuning Large Language Models (LLMs) 
using Kubernetes orchestration, demonstrating modern MLOps practices with GPU infrastructure.

## Project Overview

This project showcases how to fine-tune TinyLlama (1.1B parameters) into an instruction-following 
chat model using the Alpaca dataset, all orchestrated through Kubernetes on AWS GPU instances. 
The pipeline demonstrates memory-efficient fine-tuning with QLoRA (Quantized Low-Rank Adaptation) 
while keeping inference services running concurrently.

## Key Features

- **GPU-Accelerated Training**: Leverage NVIDIA Tesla T4 GPUs through Kubernetes for efficient model training
- **Memory-Efficient Fine-Tuning**: Implement QLoRA (4-bit quantization + LoRA adapters) to reduce GPU memory requirements by 75%
- **Production Infrastructure**: Deployed Kubernetes cluster on AWS using Kubespray with proper GPU support (NVIDIA device plugin, RuntimeClass)
- **Concurrent Operations**: Run fine-tuning jobs alongside live inference services on the same GPU
- **Containerized Workflows**: Fully Dockerized training and inference pipelines
- **Checkpoint Recovery**: Resume training after interruptions using AWS EBS
- **A/B Testing Ready**: Compare base model vs fine-tuned model performance side-by-side
- **GitOps Workflow**: Version-controlled infrastructure and configurations, datasets, and hyperparameters

## Architecture

Transform a base language model into a conversational AI through Kubernetes-orchestrated training:


AWS G4DN Instance (Tesla T4 GPU, 32GB RAM)
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                                      â”‚
                        Kubernetes Cluster (Kubespray)
                                      â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                         â”‚                         â”‚
            â–¼                         â–¼                         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   TRAINING    â”‚        â”‚  BASE MODEL   â”‚        â”‚  FINE-TUNED   â”‚
    â”‚     JOB       â”‚        â”‚   INFERENCE   â”‚        â”‚   INFERENCE   â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ â€¢ QLoRA       â”‚        â”‚ â€¢ TinyLlama   â”‚        â”‚ â€¢ TinyLlama   â”‚
    â”‚ â€¢ GPU: 1      â”‚        â”‚ â€¢ GPU: 1      â”‚        â”‚   + LoRA      â”‚
    â”‚ â€¢ Mem: 8Gi    â”‚        â”‚ â€¢ Mem: 2Gi    â”‚        â”‚ â€¢ GPU: 1      â”‚
    â”‚ â€¢ Alpaca      â”‚        â”‚ â€¢ FastAPI     â”‚        â”‚ â€¢ Mem: 2Gi    â”‚
    â”‚   Dataset     â”‚        â”‚               â”‚        â”‚ â€¢ FastAPI     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                         â”‚                         â”‚
            â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                  â”‚
            â–¼                                  â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ PERSISTENT    â”‚                â”‚  NVIDIA GPU    â”‚
    â”‚   VOLUME      â”‚                â”‚ DEVICE PLUGIN  â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚ â€¢ EBS 50Gi    â”‚                         â”‚
    â”‚ â€¢ Checkpoints â”‚                         â”‚
    â”‚ â€¢ Artifacts   â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚                                 â”‚
            â”‚                â–¼                                 â–¼
            â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚        â”‚   WEB UI      â”‚                â”‚  USER BROWSER â”‚
            â”‚        â”‚  (Nginx)      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤               â”‚
            â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ HUGGING FACE  â”‚
    â”‚     HUB       â”‚
    â”‚ (Model Store) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

External Services:
  ğŸ¤— Hugging Face Hub â—„â”€â”€â–º Training Job (push/pull models)
  ğŸŒ User Browser â”€â”€â”€â”€â”€â”€â”€â”€â–º Web UI (Nginx) â”€â”€â–º APIs

***Training Pipeline***: TinyLlama-1.1B â†’ [QLoRA + Alpaca Dataset] â†’ Fine-tuned Model â†’ HuggingFace Hub

### Technology Stack

| Component | Technology |
|-----------|------------|
| **Compute** | AWS G4DN (Tesla T4 GPU) |
| **Orchestration** | Kubernetes 1.28+ (Kubespray) |
| **Container Runtime** | containerd with NVIDIA runtime |
| **ML Framework** | PyTorch 2.0+, Transformers 4.35+ |
| **Fine-Tuning** | PEFT (QLoRA), 4-bit quantization |
| **Backend** | FastAPI, Uvicorn |
| **Frontend** | HTML/CSS/JS, Nginx |
| **Storage** | AWS EBS (gp3) |
| **Registry** | GitHub Container Registry |

## Repository Structure

â”œâ”€â”€ infrastructure/                # Kubernetes cluster setup scripts
â”œâ”€â”€ training/                      # Fine-tuning QLoRA training script and Dockerfile
â”œâ”€â”€ k8s-manifests/                 # Kubernetes resource definitions
â”‚   â”œâ”€â”€ training-instruction-fintune/
â”‚   â”‚   â”œâ”€â”€ models-pv.yaml         # PersistentVolume
â”‚   â”‚   â””â”€â”€ training-job.yaml      # Training Job
â”‚   â”œâ”€â”€ inference-base/
â”‚   â”‚   â”œâ”€â”€ deployment.yaml        # Base model deployment
â”‚   â”‚   â””â”€â”€ service.yaml           # Base model service
â”‚   â”œâ”€â”€ inference-finetuned/
â”‚   â”‚   â”œâ”€â”€ deployment.yaml        # Fine-tuned deployment
â”‚   â”‚   â””â”€â”€ service.yaml           # Fine-tuned service
â”‚   â”œâ”€â”€ ui-base/
â”‚   â”‚   â”œâ”€â”€ configmap.yaml         # Base UI HTML
â”‚   â”‚   â””â”€â”€ deployment.yaml        # Base UI deployment
â”‚   â””â”€â”€ ui-finetuned/
â”‚       â”œâ”€â”€ configmap.yaml         # Fine-tuned UI HTML
â”‚       â””â”€â”€ deployment.yaml        # Fine-tuned UI deployment
â”œâ”€â”€ inference-base-model/          # Base model FastAPI application
â”œâ”€â”€ inference-finetuned-model/     # Fine-tuned model FastAPI application
â””â”€â”€ docs/                          # Detailed documentation

## Quick Start

### Step 1: Clone Repository
```bash
git clone https://github.com/yourusername/finetuning-llm-with-k8s.git
cd finetuning-llm-with-k8s
```

### Step 2: Configure AWS Connection
Edit `infrastructure/setup_aws_node.sh` with your instance details:

```bash
PUBLIC_IP="YOUR_AWS_PUBLIC_IP"       # e.g., "54.123.45.67"
PRIVATE_IP="YOUR_AWS_PRIVATE_IP"     # e.g., "172.31.0.10"
SSH_KEY_PATH="~/.ssh/your-key.pem"   # Path to your SSH key
SSH_USER="ubuntu"                     # SSH username
```

### Step 3: Deploy Kubernetes Cluster
```bash
# Make scripts executable
chmod +x infrastructure/*.sh

# Run automated setup (takes ~30-45 minutes)
./infrastructure/setup_aws_node.sh
```

This script will:
1. Install Kubespray and dependencies
2. Deploy Kubernetes to your AWS instance
3. Configure GPU support (NVIDIA device plugin)
4. Set up kubectl on your local machine
5. Verify cluster health

### Step 4: Create Persistent Storage
```bash
# Create directory on AWS instance
ssh -i ~/.ssh/your-key.pem ubuntu@YOUR_PUBLIC_IP "sudo mkdir -p /mnt/fast-disks/models && sudo chmod 777 /mnt/fast-disks/models"

# Deploy PersistentVolume
kubectl apply -f k8s-manifests/training-instruction-fintune/models-pv.yaml

# Verify
kubectl get pv
```

### Step 5: Configure Hugging Face Credentials
```bash
# Get your token from https://huggingface.co/settings/tokens
kubectl create secret generic huggingface-token \
  --from-literal=token='hf_YOUR_ACTUAL_TOKEN' \
  --from-literal=repo='your-username/tinyllama-alpaca-finetuned'
```

### Step 6: Launch Training Job
```bash
# Deploy training job
kubectl apply -f k8s-manifests/training-instruction-fintune/training-job.yaml

# Monitor progress
kubectl logs -f job/tinyllama-finetune-alpaca
```

### Step 7: Deploy Inference Services

#### Deploy Base Model
```bash
kubectl apply -f k8s-manifests/inference-base/
```

#### Deploy Fine-Tuned Model (after training completes)
```bash
kubectl apply -f k8s-manifests/inference-finetuned/
```

#### Deploy Web UIs
```bash
# Base model UI
kubectl apply -f k8s-manifests/ui-base/

# Fine-tuned model UI
kubectl apply -f k8s-manifests/ui-finetuned/
```

### Step 8: Access Services

```bash
# Get NodePort for services
kubectl get svc

# Example output:
# llm-api-finetuned-service   NodePort   10.96.1.1   <none>   80:30557/TCP
# llm-ui-finetuned-service    NodePort   10.96.1.2   <none>   80:31234/TCP
```

Access in browser:
- **Fine-tuned API**: `http://YOUR_PUBLIC_IP:30557`
- **Fine-tuned UI**: `http://YOUR_PUBLIC_IP:31234`
